{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation based on Segmentation and Iterative Closest Point fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please install any dependencies for the notebook (see bellow). It is best to use Python 3.8 interpreter and a virtual environment (see `venv` module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from ultralytics import SAM\n",
    "from ultralytics import RTDETR\n",
    "import copy\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we read the RGB and depth maps from example files. The camera used to record these images is a Intel D435 RGBD camera, with a known intrinsic matrix (i.e. `Kdepth`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_img = cv2.imread('../asset/example_image.png')\n",
    "depth_img = cv2.imread('../asset/example_depth.png', cv2.IMREAD_ANYDEPTH)\n",
    "Kdepth = np.array([[637.22601318,   0.        , 644.54681396],\n",
    "                    [  0.        , 636.76959229, 363.35079956],\n",
    "                    [  0.        ,   0.        ,   1.        ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Color image: {color_img.dtype, color_img.shape}')\n",
    "print(f'Depth image: {depth_img.dtype, depth_img.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(color_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(depth_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Point cloud using the $K$ matrix and the depth image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create point cloud\n",
    "pts = cv2.rgbd.depthTo3d(depth_img, Kdepth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the point cloud using Open3D viewer. Please note, that the point cloud contains a lot of garbage points resulted from the violation of the minimum and maximum distance of the D435 camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize point cloud\n",
    "verts = pts.reshape((-1,3))\n",
    "idx = ~np.isnan(verts).any(axis=1)\n",
    "verts = verts[idx,:]\n",
    "color = color_img.reshape((-1,3))\n",
    "color = color[idx,:]\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "orig_pcd = pcd\n",
    "pcd.points = o3d.utility.Vector3dVector(verts)\n",
    "pcd.colors = o3d.utility.Vector3dVector(color/255)\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we find bottles in the RGB image using RTDETR object detector. After that, we save the detections and use the SAM model to segment the pixels of the detections, resulting in **masks** for the bottles we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_model = RTDETR('rtdetr-x.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cls indices for bottle and cup\n",
    "cls_idxs = [id for id,name in det_model.names.items() if name in ['bottle','cup']]\n",
    "cls_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = det_model(color_img, classes = cls_idxs)\n",
    "det_result = results[0]\n",
    "len(det_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(det_result.plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = SAM('mobile_sam.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_result = sam.predict(color_img, bboxes = det_result.boxes.xyxy)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sam_result.plot())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate the poses for the objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `DOWN_SAMPLE_SIZE`. We will use this to downsample the point clouds to 4 mm in order to speed up ICP process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWN_SAMPLE_SIZE = 4e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the reference point cloud for the object. The pose will be calculated as a transformation between the reference object and the detected objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_pcd = o3d.io.read_point_cloud(\"../asset/bottle_large.pcd\")\n",
    "cnt = np.asarray(ref_pcd.points).shape[0]\n",
    "ref_pcd.colors = o3d.utility.Vector3dVector(np.repeat([[1,0,0]],cnt,axis = 0).astype(np.float32))\n",
    "ref_pcd = ref_pcd.voxel_down_sample(DOWN_SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to estimate the pose for an object in the RGBD image filtered by a mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pose_for_mask(pts,color_img,mask):\n",
    "    \"\"\"\n",
    "    Estimate the pose for an object in the point cloud.\n",
    "    - pts: the point cloud with shape (height,width,coord) with type np.float32\n",
    "    - color_img: the colors for the pts points with shape (height,width,3)\n",
    "    - mask: mask for the object with shape (height,width) with type np.bool\n",
    "\n",
    "    Returns a tuple:\n",
    "        tvec - translation\n",
    "        rvec - Rodrigues vector for rotation\n",
    "        reg_p2p - the result object from Open3D (open3d.pipelines.registration.RegistrationResult)\n",
    "    \"\"\"\n",
    "    objpts = pts[mask,:]\n",
    "    color = color_img[mask,:]\n",
    "\n",
    "    #remove NaNs (e.g. wrong depth pixels)\n",
    "    verts = objpts.reshape((-1,3))\n",
    "    idx = ~np.isnan(verts).any(axis=1)\n",
    "    verts = verts[idx,:]\n",
    "    color = color[idx,:]\n",
    "\n",
    "    # create PCD\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(verts)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(color/255)\n",
    "\n",
    "    # downsample the PCD\n",
    "    pcd_ds = pcd.voxel_down_sample(DOWN_SAMPLE_SIZE)\n",
    "\n",
    "    # default transformation is around the mean of the object, with identity rotation\n",
    "    pts_mean = np.mean(np.asarray(pcd_ds.points),axis=0)\n",
    "    initial_transform = np.block([[np.identity(3), np.asmatrix(pts_mean).T],[0,0,0,1]])\n",
    "\n",
    "    # run pose estimation with different outlier margins\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        ref_pcd, pcd_ds, 1, initial_transform,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "        o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=100))\n",
    "\n",
    "\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        ref_pcd, pcd_ds, 0.01, reg_p2p.transformation,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "        o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=100))\n",
    "    \n",
    "    # create rvec and tvec\n",
    "    tvec = reg_p2p.transformation[0:3,3]\n",
    "    rvec,_ = cv2.Rodrigues(reg_p2p.transformation[0:3,0:3])\n",
    "    rvec = rvec.T[0,:]\n",
    "    return tvec,rvec,reg_p2p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate pose for all the objects detected in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [estimate_pose_for_mask(pts,color_img,mask.numpy()) for mask in sam_result.masks.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize result, presenting the **fitness** score for the objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase figure size for readable text\n",
    "plt.rcParams['figure.figsize'] = [15, 7]\n",
    "\n",
    "seg_debug_img = color_img.copy()\n",
    "\n",
    "for id,(mask,result) in enumerate(zip(sam_result.masks,results)):\n",
    "    mask = mask.data.numpy()[-1,:,:]\n",
    "\n",
    "    blend = result[2].fitness\n",
    "    seg_debug_img[mask,:] = (1 - blend) * seg_debug_img[mask,:] + (blend) * np.array([255,0,0])\n",
    "\n",
    "\n",
    "for id,(mask,result) in enumerate(zip(sam_result.masks,results)):\n",
    "    crd = np.mean(mask.xy,axis=1).astype(np.int32).flatten().tolist()\n",
    "    text = f\"{id} - {result[2].fitness:0.2} - {result[2].inlier_rmse:0.3}\"\n",
    "    cv2.putText(seg_debug_img,org=crd,text=text,fontFace = 0, fontScale = 0.4, color = (0,255,0), thickness = 1)\n",
    "\n",
    "plt.imshow(seg_debug_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the results by **fitness** score. Fitness ranges from 0 to 1, and shows the inlier proportion. For an object, even 0.5 can be a successful match, since the object can be seen from one side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FITNESS_LIMIT = 0.4\n",
    "filtered_results = filter(lambda x: x[2].fitness > FITNESS_LIMIT, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ref_point_cloud(transform):\n",
    "    ref_pcd_temp = copy.deepcopy(ref_pcd)\n",
    "    return ref_pcd_temp.transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_all = [generate_ref_point_cloud(reg_res.transformation) for _,_,reg_res in filtered_results]\n",
    "o3d.visualization.draw_geometries([pcd, *pcd_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best by fitness\n",
    "all_fitness = [res[2].fitness for res in results]\n",
    "best_pcd_idx = all_fitness.index(max(all_fitness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_best = generate_ref_point_cloud(results[best_pcd_idx][2].transformation)\n",
    "o3d.visualization.draw_geometries([pcd, pcd_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
